{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassClassifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajnryt/JerusMLDeepLearning2019/blob/master/ClassClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvEbIcsEMonV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "09a51489-b340-428f-de5e-65b7a6edeba4"
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQtzYuUM88R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "class InstanceDataset(Dataset):\n",
        "    def __init__(self, basedir, transform=None):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'*','*','*.jpg'))\n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in files], \n",
        "                            columns=['class_id', 'file_path'])\n",
        "        \n",
        "    def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return int(parts[0]), f   #label is originaly a str\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = imageio.imread(dat['file_path'])\n",
        "      img = np.resize(img,(3,128,128))\n",
        "      return (torch.from_numpy(img.astype(np.float32)), dat['class_id'])\n",
        "              \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le4IlqXFsrZo",
        "colab_type": "code",
        "outputId": "a04911a7-2c23-4a63-e292-1f6760c742c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "basedir = '/content/gdrive/My Drive/videos_2/yt_bb_detection_train'\n",
        "\n",
        "# data_transforms = transforms.Compose([transforms.Resize(128,128), \n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "#Create DS\n",
        "dataset = InstanceDataset(basedir)\n",
        "\n",
        "len(np.unique(dataset.data['class_id']))  #how many classes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZBZOwY1Ra4",
        "colab_type": "code",
        "outputId": "945cef3d-f174-4b60-9247-cacd718461e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#Divide data to train/test\n",
        "dataset = InstanceDataset(basedir)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 1585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B45pmKnJqWZr",
        "colab_type": "code",
        "outputId": "b915f724-9b57-457b-c4fd-60afbc0870ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "import time\n",
        "tf = time.time()  \n",
        "\n",
        "#model = LeNet()\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "if use_cuda:\n",
        "   model = model.cuda()\n",
        "    \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "predicts=[]\n",
        "trainloss = []\n",
        "testloss = []\n",
        "\n",
        "for epoch in range(10):\n",
        "    # trainning\n",
        "    ave_loss = 0\n",
        "\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            print(x.shape)\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target) \n",
        "        \n",
        "        print(f'loss: {loss}')\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss))    \n",
        "            \n",
        "    trainloss.append(loss.data)\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        #x, target = Variable(x, volatile=True), Variable(target, volatile=True)\n",
        "        out = model(x)        \n",
        "        loss = criterion(out, target)\n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss,float(correct_cnt)/total_cnt))\n",
        "          \n",
        "    testloss.append(loss.data)  \n",
        "\n",
        "torch.save(model.state_dict(), model.name())\n",
        "\n",
        "elapsed = time.time() - tf\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 46827520/46827520 [00:00<00:00, 82990907.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 12.33039665222168\n",
            "loss: 12.317448616027832\n",
            "loss: 11.421674728393555\n",
            "loss: 10.626917839050293\n",
            "loss: 10.964181900024414\n",
            "loss: 9.441967964172363\n",
            "loss: 8.572966575622559\n",
            "loss: 9.568422317504883\n",
            "loss: 7.968695163726807\n",
            "loss: 7.858793258666992\n",
            "loss: 6.768931865692139\n",
            "loss: 7.003999710083008\n",
            "loss: 6.945380210876465\n",
            "loss: 6.535125255584717\n",
            "loss: 5.2747578620910645\n",
            "loss: 5.725508213043213\n",
            "loss: 5.30656623840332\n",
            "loss: 4.049806594848633\n",
            "loss: 3.8581650257110596\n",
            "loss: 4.873569488525391\n",
            "loss: 3.539717197418213\n",
            "loss: 4.242822170257568\n",
            "loss: 3.580151319503784\n",
            "loss: 3.5226523876190186\n",
            "loss: 4.0746612548828125\n",
            "loss: 3.5000689029693604\n",
            "loss: 3.0589439868927\n",
            "loss: 3.397324323654175\n",
            "loss: 3.605821132659912\n",
            "loss: 3.6771020889282227\n",
            "loss: 3.425966262817383\n",
            "loss: 3.1780166625976562\n",
            "loss: 3.0871570110321045\n",
            "loss: 2.9692795276641846\n",
            "loss: 3.445624589920044\n",
            "loss: 3.1561596393585205\n",
            "loss: 3.134550094604492\n",
            "loss: 3.1377387046813965\n",
            "loss: 2.997685432434082\n",
            "loss: 3.292421817779541\n",
            "loss: 2.9317758083343506\n",
            "loss: 2.9273993968963623\n",
            "loss: 2.577108383178711\n",
            "loss: 2.5813827514648438\n",
            "loss: 2.9518380165100098\n",
            "loss: 3.051157236099243\n",
            "loss: 2.6637461185455322\n",
            "loss: 3.2376697063446045\n",
            "loss: 2.9019668102264404\n",
            "loss: 2.8370020389556885\n",
            "loss: 2.7971324920654297\n",
            "loss: 2.9196362495422363\n",
            "loss: 2.9293150901794434\n",
            "loss: 2.918355703353882\n",
            "loss: 2.795738935470581\n",
            "loss: 2.9175865650177\n",
            "loss: 2.6330716609954834\n",
            "loss: 2.875528573989868\n",
            "loss: 2.9712822437286377\n",
            "loss: 2.7520751953125\n",
            "loss: 2.9141268730163574\n",
            "loss: 2.94624662399292\n",
            "loss: 2.669332504272461\n",
            "loss: 3.027177095413208\n",
            "loss: 3.0668530464172363\n",
            "loss: 2.7325704097747803\n",
            "loss: 2.6652395725250244\n",
            "loss: 2.7237298488616943\n",
            "loss: 2.6208274364471436\n",
            "loss: 2.808845281600952\n",
            "loss: 2.765343427658081\n",
            "loss: 2.6023035049438477\n",
            "loss: 3.0494134426116943\n",
            "loss: 2.8407034873962402\n",
            "loss: 2.7114980220794678\n",
            "loss: 3.1292552947998047\n",
            "loss: 2.7771432399749756\n",
            "loss: 2.7057158946990967\n",
            "loss: 2.7988805770874023\n",
            "loss: 2.6748268604278564\n",
            "loss: 2.968710422515869\n",
            "loss: 2.621868848800659\n",
            "loss: 3.0007004737854004\n",
            "loss: 2.725278615951538\n",
            "loss: 2.4810030460357666\n",
            "loss: 2.64959716796875\n",
            "loss: 2.6661221981048584\n",
            "loss: 2.9624009132385254\n",
            "loss: 2.8400332927703857\n",
            "loss: 2.725065231323242\n",
            "loss: 3.02384614944458\n",
            "loss: 2.666788101196289\n",
            "loss: 2.7289421558380127\n",
            "loss: 2.7850615978240967\n",
            "loss: 2.592193365097046\n",
            "loss: 2.718459129333496\n",
            "loss: 2.626310110092163\n",
            "loss: 2.661874771118164\n",
            "loss: 2.8653125762939453\n",
            "loss: 2.9501256942749023\n",
            "==>>> epoch: 0, batch index: 100, train loss: 2.771343\n",
            "loss: 2.620401620864868\n",
            "loss: 2.721423864364624\n",
            "loss: 2.5461645126342773\n",
            "loss: 2.7463178634643555\n",
            "loss: 2.7890195846557617\n",
            "loss: 2.444040298461914\n",
            "loss: 2.46842360496521\n",
            "loss: 2.831824541091919\n",
            "loss: 2.4012255668640137\n",
            "loss: 2.711439847946167\n",
            "loss: 2.8885581493377686\n",
            "loss: 2.824655532836914\n",
            "loss: 2.329169511795044\n",
            "loss: 2.7540433406829834\n",
            "loss: 2.4369707107543945\n",
            "loss: 2.410693645477295\n",
            "loss: 2.7212510108947754\n",
            "loss: 2.743485927581787\n",
            "loss: 2.6389684677124023\n",
            "loss: 2.9799909591674805\n",
            "loss: 2.7565600872039795\n",
            "loss: 2.891831874847412\n",
            "loss: 2.8109707832336426\n",
            "loss: 2.6068379878997803\n",
            "loss: 2.6463425159454346\n",
            "loss: 2.6515982151031494\n",
            "loss: 2.6545119285583496\n",
            "loss: 2.5177676677703857\n",
            "loss: 2.672804594039917\n",
            "loss: 2.493546724319458\n",
            "loss: 2.568269968032837\n",
            "loss: 2.5041794776916504\n",
            "loss: 2.4940476417541504\n",
            "loss: 2.815387010574341\n",
            "loss: 2.7330820560455322\n",
            "loss: 2.604271173477173\n",
            "loss: 2.3673343658447266\n",
            "loss: 2.738962173461914\n",
            "loss: 2.7444353103637695\n",
            "loss: 2.5544164180755615\n",
            "loss: 2.5663115978240967\n",
            "loss: 2.789278268814087\n",
            "loss: 2.7804808616638184\n",
            "loss: 2.7957863807678223\n",
            "loss: 2.71498966217041\n",
            "loss: 2.798042058944702\n",
            "loss: 2.2747750282287598\n",
            "loss: 2.7404427528381348\n",
            "loss: 2.5438613891601562\n",
            "loss: 2.408712387084961\n",
            "loss: 2.7872040271759033\n",
            "loss: 2.3357126712799072\n",
            "loss: 2.663888931274414\n",
            "loss: 2.4120895862579346\n",
            "loss: 2.656834602355957\n",
            "loss: 2.487187385559082\n",
            "loss: 2.8894762992858887\n",
            "loss: 2.956453561782837\n",
            "loss: 2.6363394260406494\n",
            "loss: 2.7670071125030518\n",
            "loss: 2.759446144104004\n",
            "loss: 2.669407367706299\n",
            "loss: 2.6071383953094482\n",
            "loss: 2.86616587638855\n",
            "loss: 2.895855665206909\n",
            "loss: 2.7111005783081055\n",
            "loss: 2.3620986938476562\n",
            "loss: 2.7638814449310303\n",
            "loss: 2.892679214477539\n",
            "loss: 2.6084113121032715\n",
            "loss: 2.427198886871338\n",
            "loss: 2.7258694171905518\n",
            "loss: 2.785576105117798\n",
            "loss: 2.5167832374572754\n",
            "loss: 2.6481130123138428\n",
            "loss: 2.60994029045105\n",
            "loss: 2.418919324874878\n",
            "loss: 2.8390655517578125\n",
            "loss: 2.877872943878174\n",
            "loss: 2.464376449584961\n",
            "loss: 2.549966335296631\n",
            "loss: 2.6995489597320557\n",
            "loss: 2.624934434890747\n",
            "loss: 2.9680118560791016\n",
            "loss: 2.7093515396118164\n",
            "loss: 2.9688820838928223\n",
            "loss: 2.54909610748291\n",
            "loss: 2.641693353652954\n",
            "loss: 2.7863237857818604\n",
            "loss: 2.5190584659576416\n",
            "loss: 2.4992306232452393\n",
            "loss: 2.339456558227539\n",
            "loss: 2.648897409439087\n",
            "loss: 2.5498123168945312\n",
            "loss: 2.4993624687194824\n",
            "loss: 3.1887357234954834\n",
            "loss: 2.5384514331817627\n",
            "loss: 2.524813652038574\n",
            "loss: 2.603513717651367\n",
            "loss: 2.595759630203247\n",
            "==>>> epoch: 0, batch index: 200, train loss: 2.635102\n",
            "loss: 2.7306439876556396\n",
            "loss: 2.523043155670166\n",
            "loss: 2.703861951828003\n",
            "loss: 2.620997428894043\n",
            "loss: 2.708599805831909\n",
            "loss: 2.6393613815307617\n",
            "loss: 2.8731021881103516\n",
            "loss: 2.559518814086914\n",
            "loss: 2.5637640953063965\n",
            "loss: 2.6253654956817627\n",
            "loss: 2.676542282104492\n",
            "loss: 2.6555261611938477\n",
            "loss: 2.830625057220459\n",
            "loss: 2.7167739868164062\n",
            "loss: 2.922778367996216\n",
            "loss: 2.5149545669555664\n",
            "loss: 2.390942335128784\n",
            "loss: 2.537961959838867\n",
            "loss: 2.8493640422821045\n",
            "loss: 2.4169678688049316\n",
            "loss: 2.6582648754119873\n",
            "loss: 2.5364484786987305\n",
            "loss: 2.7288918495178223\n",
            "loss: 2.821608543395996\n",
            "loss: 3.099668502807617\n",
            "loss: 2.4613170623779297\n",
            "loss: 2.6655495166778564\n",
            "loss: 2.81721830368042\n",
            "loss: 2.4523322582244873\n",
            "loss: 2.4299819469451904\n",
            "loss: 2.6463515758514404\n",
            "loss: 2.4011881351470947\n",
            "loss: 2.356947422027588\n",
            "loss: 2.567440986633301\n",
            "loss: 2.848625421524048\n",
            "loss: 2.4175186157226562\n",
            "loss: 2.722550392150879\n",
            "loss: 2.42976450920105\n",
            "loss: 2.6683766841888428\n",
            "loss: 2.412295341491699\n",
            "loss: 2.488365888595581\n",
            "loss: 2.7762856483459473\n",
            "loss: 2.7176153659820557\n",
            "loss: 2.488438367843628\n",
            "loss: 2.5041773319244385\n",
            "loss: 2.7037689685821533\n",
            "loss: 2.4437949657440186\n",
            "loss: 2.6975815296173096\n",
            "loss: 2.6253597736358643\n",
            "loss: 2.6360745429992676\n",
            "loss: 2.765317916870117\n",
            "loss: 2.662853717803955\n",
            "loss: 2.4024219512939453\n",
            "loss: 2.478769540786743\n",
            "loss: 2.8683249950408936\n",
            "loss: 2.588881015777588\n",
            "loss: 2.8698601722717285\n",
            "loss: 2.3457350730895996\n",
            "loss: 2.559006452560425\n",
            "loss: 2.5888402462005615\n",
            "loss: 2.793752670288086\n",
            "loss: 2.576245069503784\n",
            "loss: 2.439696788787842\n",
            "loss: 2.650939702987671\n",
            "loss: 2.823585271835327\n",
            "loss: 2.7667009830474854\n",
            "loss: 2.6598005294799805\n",
            "loss: 2.848679542541504\n",
            "loss: 2.5396664142608643\n",
            "loss: 2.571831464767456\n",
            "loss: 2.473663091659546\n",
            "loss: 2.330453872680664\n",
            "loss: 2.365227699279785\n",
            "loss: 2.662562847137451\n",
            "loss: 2.4851014614105225\n",
            "loss: 2.707125663757324\n",
            "loss: 2.7667016983032227\n",
            "loss: 2.384190559387207\n",
            "loss: 2.5731213092803955\n",
            "loss: 2.2801599502563477\n",
            "loss: 2.5023841857910156\n",
            "loss: 2.530517339706421\n",
            "loss: 2.8941457271575928\n",
            "loss: 2.364173173904419\n",
            "loss: 2.740407705307007\n",
            "loss: 2.7174007892608643\n",
            "loss: 2.4945478439331055\n",
            "loss: 2.5378167629241943\n",
            "loss: 2.6218087673187256\n",
            "loss: 2.5955779552459717\n",
            "loss: 2.525062322616577\n",
            "loss: 2.452390193939209\n",
            "loss: 2.705118179321289\n",
            "loss: 2.34580135345459\n",
            "loss: 2.356605052947998\n",
            "loss: 2.336846351623535\n",
            "loss: 2.444052219390869\n",
            "loss: 2.300384759902954\n",
            "loss: 2.6378588676452637\n",
            "loss: 2.257883071899414\n",
            "==>>> epoch: 0, batch index: 300, train loss: 2.478071\n",
            "loss: 3.011491060256958\n",
            "loss: 2.9190750122070312\n",
            "loss: 2.1532468795776367\n",
            "loss: 2.6327812671661377\n",
            "loss: 2.420595407485962\n",
            "loss: 2.5890657901763916\n",
            "loss: 3.0396835803985596\n",
            "loss: 2.5516061782836914\n",
            "loss: 2.4592413902282715\n",
            "loss: 2.6128721237182617\n",
            "loss: 2.6524202823638916\n",
            "loss: 2.6464760303497314\n",
            "loss: 2.5926647186279297\n",
            "loss: 2.368393898010254\n",
            "loss: 2.8173773288726807\n",
            "loss: 2.300644874572754\n",
            "loss: 2.8569653034210205\n",
            "loss: 2.6936140060424805\n",
            "loss: 2.1967644691467285\n",
            "loss: 2.2854433059692383\n",
            "loss: 2.5980870723724365\n",
            "loss: 2.3884823322296143\n",
            "loss: 2.7651193141937256\n",
            "loss: 2.470487356185913\n",
            "loss: 2.5869152545928955\n",
            "loss: 2.6326799392700195\n",
            "loss: 2.286940813064575\n",
            "loss: 2.4694416522979736\n",
            "loss: 2.8526456356048584\n",
            "loss: 2.4543893337249756\n",
            "loss: 2.5061533451080322\n",
            "loss: 2.685434341430664\n",
            "loss: 2.622126817703247\n",
            "loss: 2.4802091121673584\n",
            "loss: 2.4165143966674805\n",
            "loss: 2.277332067489624\n",
            "loss: 2.542548418045044\n",
            "loss: 2.8459928035736084\n",
            "loss: 2.506812810897827\n",
            "loss: 2.4895026683807373\n",
            "loss: 2.352149248123169\n",
            "loss: 2.7978579998016357\n",
            "loss: 2.4716811180114746\n",
            "loss: 2.407207727432251\n",
            "loss: 2.3797338008880615\n",
            "loss: 2.43941068649292\n",
            "loss: 2.6965279579162598\n",
            "loss: 2.522165298461914\n",
            "loss: 2.4358882904052734\n",
            "loss: 2.350236415863037\n",
            "loss: 2.5642027854919434\n",
            "loss: 2.3975417613983154\n",
            "loss: 2.403092622756958\n",
            "loss: 2.9051852226257324\n",
            "loss: 2.474682569503784\n",
            "loss: 2.5160858631134033\n",
            "loss: 2.650235891342163\n",
            "loss: 2.3508689403533936\n",
            "loss: 2.5726630687713623\n",
            "loss: 2.736772060394287\n",
            "loss: 2.3888638019561768\n",
            "loss: 2.7250661849975586\n",
            "loss: 2.692500114440918\n",
            "loss: 2.4912171363830566\n",
            "loss: 2.6145763397216797\n",
            "loss: 2.654437780380249\n",
            "loss: 2.88041353225708\n",
            "loss: 2.6826119422912598\n",
            "loss: 2.9645743370056152\n",
            "loss: 2.4670228958129883\n",
            "loss: 2.4701080322265625\n",
            "loss: 2.4039430618286133\n",
            "loss: 2.40815806388855\n",
            "loss: 2.306983232498169\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}