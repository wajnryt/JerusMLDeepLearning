{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstanceClassifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajnryt/JerusMLDeepLearning2019/blob/master/InstanceClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvEbIcsEMonV",
        "colab_type": "code",
        "outputId": "5e257954-69fa-49c6-fe99-cce5dc54c96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQtzYuUM88R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def create_instance_hashmap():\n",
        "      index = 1\n",
        "      names  = np.unique(dataset.data['instance_id'])\n",
        "      instanceDict = {}\n",
        "      for str in names:\n",
        "        instanceDict[str] = index\n",
        "        index = index+1\n",
        "        print (str, index)\n",
        "        \n",
        "\n",
        "\n",
        "class InstanceDataset(Dataset):\n",
        "          \n",
        "  \n",
        "    def __init__(self, basedir, transform=None):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'*','*','*.jpg'))\n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in files], \n",
        "                            columns=['instance_id', 'file_path'])\n",
        "        index = 0\n",
        "        names  = np.unique(self.data['instance_id'])\n",
        "        self.instanceDict = {}\n",
        "        for str in names:\n",
        "          self.instanceDict[str] = index\n",
        "          index = index+1\n",
        "          \n",
        "        self.data['instance_num'] = self.data['instance_id'].map(self.instanceDict)\n",
        "          \n",
        "    \n",
        "        \n",
        "    def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[1], f   #label is originaly a str\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = imageio.imread(dat['file_path'])\n",
        "      img = np.resize(img,(3,128,128))\n",
        "      return (torch.from_numpy(img.astype(np.float32)), dat['instance_num'])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "          \n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgdhhhjcwc1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "basedir = '/content/gdrive/My Drive/videos_2/yt_bb_detection_train'       \n",
        "        \n",
        "dataset = InstanceDataset(basedir)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nkVWqUqjEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39fc5a3a-49c6-45b3-e964-527bfce4682b"
      },
      "source": [
        "len((dataset.data['instance_num']))\n",
        "len(np.unique(dataset.data['instance_num']))\n",
        "dataset.data['instance_num'][200]\n",
        "max(np.unique(dataset.data['instance_num']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4907"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZBZOwY1Ra4",
        "colab_type": "code",
        "outputId": "98468df2-d821-4788-dce9-eff3297f6f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#Divide data to train/test\n",
        "dataset = InstanceDataset(basedir)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 1585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNxdoCQ5Y7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.conv3 = nn.Conv2d(50,60,6,1)\n",
        "        self.fc1 = nn.Linear(34560, 10000)\n",
        "        self.fc2 = nn.Linear(10000, 6476)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 34560)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def name(self):\n",
        "        return \"LeNet\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B45pmKnJqWZr",
        "colab_type": "code",
        "outputId": "334f42c0-d940-43f5-97a4-a727baa67b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter()    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES = len(np.unique(dataset.data['instance_num']))\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES)        \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "predicts=[]\n",
        "trainloss = []\n",
        "testloss = []\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    # trainning\n",
        "    ave_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        tf = time.time() \n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target) \n",
        "        \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "               \n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, ave_loss, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "        trainloss.append(loss.data)\n",
        "        writer.add_text('train accuracy', 'train accuracy: ' + str(float(correct_cnt)/total_cnt))\n",
        "        writer.add_text('train loss', 'train loss: ' + str(loss.data))\n",
        "        \n",
        "        elapsed = time.time() - tf\n",
        "        print(f'loss: {loss}      Elapsed time: {elapsed}')\n",
        "\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss,float(correct_cnt)/total_cnt))\n",
        "          \n",
        "        testloss.append(loss.data) \n",
        "        writer.add_text('test accuracy', 'test accuracy: ' + str(float(correct_cnt)/total_cnt))\n",
        "        writer.add_text('test loss', 'test loss: ' + str(loss.data))\n",
        "#         pl.figure(1)\n",
        "#         test_handle, = plt.plot(testloss, 'bo-')\n",
        "#         pl.show()\n",
        "\n",
        "torch.save(model.state_dict(), model.name())\n",
        "writer.close()\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 9.067075729370117      Elapsed time: 0.1139833927154541\n",
            "loss: 8.887372970581055      Elapsed time: 0.11622953414916992\n",
            "loss: 9.230437278747559      Elapsed time: 0.11677789688110352\n",
            "loss: 9.3069486618042      Elapsed time: 0.11424994468688965\n",
            "loss: 9.25303840637207      Elapsed time: 0.11624288558959961\n",
            "loss: 9.223797798156738      Elapsed time: 0.11571502685546875\n",
            "loss: 9.152804374694824      Elapsed time: 0.11592626571655273\n",
            "loss: 9.0516939163208      Elapsed time: 0.11623334884643555\n",
            "loss: 9.07166576385498      Elapsed time: 0.11616349220275879\n",
            "loss: 9.103982925415039      Elapsed time: 0.11628389358520508\n",
            "loss: 9.107826232910156      Elapsed time: 0.11603569984436035\n",
            "loss: 9.204618453979492      Elapsed time: 0.1165318489074707\n",
            "loss: 9.20289421081543      Elapsed time: 0.11529898643493652\n",
            "loss: 9.126773834228516      Elapsed time: 0.11614274978637695\n",
            "loss: 9.0743989944458      Elapsed time: 0.11594605445861816\n",
            "loss: 9.082277297973633      Elapsed time: 0.11712956428527832\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}