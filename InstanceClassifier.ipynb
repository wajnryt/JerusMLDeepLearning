{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstanceClassifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajnryt/JerusMLDeepLearning2019/blob/Rachel/InstanceClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvEbIcsEMonV",
        "colab_type": "code",
        "outputId": "5e257954-69fa-49c6-fe99-cce5dc54c96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQtzYuUM88R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def create_instance_hashmap():\n",
        "      index = 1\n",
        "      names  = np.unique(dataset.data['instance_id'])\n",
        "      instanceDict = {}\n",
        "      for str in names:\n",
        "        instanceDict[str] = index\n",
        "        index = index+1\n",
        "        print (str, index)\n",
        "        \n",
        "\n",
        "\n",
        "class InstanceDataset(Dataset):\n",
        "          \n",
        "  \n",
        "    def __init__(self, basedir, transform=None):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'*','*','*.jpg'))\n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in files], \n",
        "                            columns=['instance_id', 'file_path'])\n",
        "        index = 0\n",
        "        names  = np.unique(self.data['instance_id'])\n",
        "        self.instanceDict = {}\n",
        "        for str in names:\n",
        "          self.instanceDict[str] = index\n",
        "          index = index+1\n",
        "          \n",
        "        self.data['instance_num'] = self.data['instance_id'].map(self.instanceDict)\n",
        "          \n",
        "    \n",
        "        \n",
        "    def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[1], f   #label is originaly a str\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = imageio.imread(dat['file_path'])\n",
        "      img = np.resize(img,(3,128,128))\n",
        "      return (torch.from_numpy(img.astype(np.float32)), dat['instance_num'])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "          \n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgdhhhjcwc1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "basedir = '/content/gdrive/My Drive/videos_2/yt_bb_detection_train'       \n",
        "        \n",
        "dataset = InstanceDataset(basedir)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nkVWqUqjEy",
        "colab_type": "code",
        "outputId": "39fc5a3a-49c6-45b3-e964-527bfce4682b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len((dataset.data['instance_num']))\n",
        "len(np.unique(dataset.data['instance_num']))\n",
        "dataset.data['instance_num'][200]\n",
        "max(np.unique(dataset.data['instance_num']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4907"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZBZOwY1Ra4",
        "colab_type": "code",
        "outputId": "98468df2-d821-4788-dce9-eff3297f6f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#Divide data to train/test\n",
        "dataset = InstanceDataset(basedir)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 1585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNxdoCQ5Y7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.conv3 = nn.Conv2d(50,60,6,1)\n",
        "        self.fc1 = nn.Linear(34560, 10000)\n",
        "        self.fc2 = nn.Linear(10000, 6476)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 34560)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def name(self):\n",
        "        return \"LeNet\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B45pmKnJqWZr",
        "colab_type": "code",
        "outputId": "abbf3ca5-e77f-49f6-ce0c-8347b1c35089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter()    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES = len(np.unique(dataset.data['instance_num']))\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES)        \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "predicts=[]\n",
        "trainloss = []\n",
        "testloss = []\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    # trainning\n",
        "    ave_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        tf = time.time() \n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target) \n",
        "        \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "               \n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, ave_loss, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "        trainloss.append(loss.data)\n",
        "        writer.add_text('train accuracy', 'train accuracy: ' + str(float(correct_cnt)/total_cnt))\n",
        "        writer.add_text('train loss', 'train loss: ' + str(loss.data))\n",
        "        \n",
        "        elapsed = time.time() - tf\n",
        "        print(f'loss: {loss}      Elapsed time: {elapsed}')\n",
        "\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss,float(correct_cnt)/total_cnt))\n",
        "          \n",
        "        testloss.append(loss.data) \n",
        "        writer.add_text('test accuracy', 'test accuracy: ' + str(float(correct_cnt)/total_cnt))\n",
        "        writer.add_text('test loss', 'test loss: ' + str(loss.data))\n",
        "#         pl.figure(1)\n",
        "#         test_handle, = plt.plot(testloss, 'bo-')\n",
        "#         pl.show()\n",
        "\n",
        "torch.save(model.state_dict(), model.name())\n",
        "writer.close()\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 9.067075729370117      Elapsed time: 0.1139833927154541\n",
            "loss: 8.887372970581055      Elapsed time: 0.11622953414916992\n",
            "loss: 9.230437278747559      Elapsed time: 0.11677789688110352\n",
            "loss: 9.3069486618042      Elapsed time: 0.11424994468688965\n",
            "loss: 9.25303840637207      Elapsed time: 0.11624288558959961\n",
            "loss: 9.223797798156738      Elapsed time: 0.11571502685546875\n",
            "loss: 9.152804374694824      Elapsed time: 0.11592626571655273\n",
            "loss: 9.0516939163208      Elapsed time: 0.11623334884643555\n",
            "loss: 9.07166576385498      Elapsed time: 0.11616349220275879\n",
            "loss: 9.103982925415039      Elapsed time: 0.11628389358520508\n",
            "loss: 9.107826232910156      Elapsed time: 0.11603569984436035\n",
            "loss: 9.204618453979492      Elapsed time: 0.1165318489074707\n",
            "loss: 9.20289421081543      Elapsed time: 0.11529898643493652\n",
            "loss: 9.126773834228516      Elapsed time: 0.11614274978637695\n",
            "loss: 9.0743989944458      Elapsed time: 0.11594605445861816\n",
            "loss: 9.082277297973633      Elapsed time: 0.11712956428527832\n",
            "loss: 9.002832412719727      Elapsed time: 0.11450457572937012\n",
            "loss: 9.085165023803711      Elapsed time: 0.11578536033630371\n",
            "loss: 8.995168685913086      Elapsed time: 0.11578106880187988\n",
            "loss: 9.157272338867188      Elapsed time: 0.11663818359375\n",
            "loss: 8.972790718078613      Elapsed time: 0.11640071868896484\n",
            "loss: 9.176645278930664      Elapsed time: 0.11536550521850586\n",
            "loss: 9.089334487915039      Elapsed time: 0.11647176742553711\n",
            "loss: 8.878063201904297      Elapsed time: 0.11646080017089844\n",
            "loss: 9.045438766479492      Elapsed time: 0.11629152297973633\n",
            "loss: 9.062618255615234      Elapsed time: 0.1158754825592041\n",
            "loss: 8.968975067138672      Elapsed time: 0.11653566360473633\n",
            "loss: 9.030821800231934      Elapsed time: 0.11558818817138672\n",
            "loss: 8.960636138916016      Elapsed time: 0.11641526222229004\n",
            "loss: 8.901703834533691      Elapsed time: 0.11566352844238281\n",
            "loss: 9.031028747558594      Elapsed time: 0.11650848388671875\n",
            "loss: 9.038082122802734      Elapsed time: 0.1148383617401123\n",
            "loss: 9.187259674072266      Elapsed time: 0.11651229858398438\n",
            "loss: 9.007511138916016      Elapsed time: 0.11583280563354492\n",
            "loss: 8.900246620178223      Elapsed time: 0.11683988571166992\n",
            "loss: 9.174145698547363      Elapsed time: 0.116668701171875\n",
            "loss: 8.9908447265625      Elapsed time: 0.11587643623352051\n",
            "loss: 8.94648265838623      Elapsed time: 0.11635088920593262\n",
            "loss: 8.844844818115234      Elapsed time: 0.11549234390258789\n",
            "loss: 8.750016212463379      Elapsed time: 0.11667418479919434\n",
            "loss: 8.939827919006348      Elapsed time: 0.11774873733520508\n",
            "loss: 9.0843505859375      Elapsed time: 0.11644768714904785\n",
            "loss: 8.787914276123047      Elapsed time: 0.116668701171875\n",
            "loss: 8.955009460449219      Elapsed time: 0.11531805992126465\n",
            "loss: 9.231307029724121      Elapsed time: 0.11630010604858398\n",
            "loss: 8.968903541564941      Elapsed time: 0.11614012718200684\n",
            "loss: 9.109088897705078      Elapsed time: 0.11434769630432129\n",
            "loss: 9.104185104370117      Elapsed time: 0.11650896072387695\n",
            "loss: 8.933028221130371      Elapsed time: 0.11659121513366699\n",
            "loss: 9.041872024536133      Elapsed time: 0.11612462997436523\n",
            "loss: 9.182038307189941      Elapsed time: 0.11562275886535645\n",
            "loss: 8.977121353149414      Elapsed time: 0.11625862121582031\n",
            "loss: 9.021374702453613      Elapsed time: 0.11568570137023926\n",
            "loss: 9.185956001281738      Elapsed time: 0.11617684364318848\n",
            "loss: 9.037907600402832      Elapsed time: 0.11587190628051758\n",
            "loss: 9.166023254394531      Elapsed time: 0.11655473709106445\n",
            "loss: 9.013042449951172      Elapsed time: 0.11662459373474121\n",
            "loss: 9.1264009475708      Elapsed time: 0.11618971824645996\n",
            "loss: 8.752079010009766      Elapsed time: 0.11648178100585938\n",
            "loss: 9.062586784362793      Elapsed time: 0.11616253852844238\n",
            "loss: 8.856200218200684      Elapsed time: 0.11667084693908691\n",
            "loss: 9.114910125732422      Elapsed time: 0.115936279296875\n",
            "loss: 9.249403953552246      Elapsed time: 0.11629867553710938\n",
            "loss: 9.064916610717773      Elapsed time: 0.11587119102478027\n",
            "loss: 9.146289825439453      Elapsed time: 0.11597609519958496\n",
            "loss: 9.056537628173828      Elapsed time: 0.11606001853942871\n",
            "loss: 8.934194564819336      Elapsed time: 0.11638712882995605\n",
            "loss: 8.95937442779541      Elapsed time: 0.1159360408782959\n",
            "loss: 9.028087615966797      Elapsed time: 0.11612248420715332\n",
            "loss: 9.042223930358887      Elapsed time: 0.11599874496459961\n",
            "loss: 8.942811012268066      Elapsed time: 0.11581039428710938\n",
            "loss: 9.047904014587402      Elapsed time: 0.11672282218933105\n",
            "loss: 9.093852996826172      Elapsed time: 0.1166377067565918\n",
            "loss: 9.017075538635254      Elapsed time: 0.11603140830993652\n",
            "loss: 8.922192573547363      Elapsed time: 0.11447954177856445\n",
            "loss: 9.05886459350586      Elapsed time: 0.11564898490905762\n",
            "loss: 8.918696403503418      Elapsed time: 0.11530756950378418\n",
            "loss: 9.029696464538574      Elapsed time: 0.11591792106628418\n",
            "loss: 8.818835258483887      Elapsed time: 0.11588644981384277\n",
            "loss: 8.829238891601562      Elapsed time: 0.11583995819091797\n",
            "loss: 9.091002464294434      Elapsed time: 0.11662149429321289\n",
            "loss: 8.92371654510498      Elapsed time: 0.11596989631652832\n",
            "loss: 8.928201675415039      Elapsed time: 0.11644697189331055\n",
            "loss: 8.861639022827148      Elapsed time: 0.1161046028137207\n",
            "loss: 9.075521469116211      Elapsed time: 0.11569046974182129\n",
            "loss: 8.872350692749023      Elapsed time: 0.11486029624938965\n",
            "loss: 8.825377464294434      Elapsed time: 0.1160125732421875\n",
            "loss: 8.774275779724121      Elapsed time: 0.11597752571105957\n",
            "loss: 9.177303314208984      Elapsed time: 0.11592698097229004\n",
            "loss: 8.755306243896484      Elapsed time: 0.11599612236022949\n",
            "loss: 8.85694694519043      Elapsed time: 0.11596274375915527\n",
            "loss: 9.062250137329102      Elapsed time: 0.11580538749694824\n",
            "loss: 9.037020683288574      Elapsed time: 0.1164548397064209\n",
            "loss: 9.08281135559082      Elapsed time: 0.11598944664001465\n",
            "loss: 8.890907287597656      Elapsed time: 0.11603617668151855\n",
            "loss: 8.855696678161621      Elapsed time: 0.11610269546508789\n",
            "loss: 9.049921989440918      Elapsed time: 0.11582279205322266\n",
            "loss: 8.89195728302002      Elapsed time: 0.11623668670654297\n",
            "loss: 9.111806869506836      Elapsed time: 0.11611557006835938\n",
            "==>>> epoch: 0, batch index: 100, train loss: 8.968063, acc: 0.0004\n",
            "loss: 8.972043991088867      Elapsed time: 0.11514019966125488\n",
            "loss: 8.987545013427734      Elapsed time: 0.115997314453125\n",
            "loss: 8.831774711608887      Elapsed time: 0.11501240730285645\n",
            "loss: 8.871801376342773      Elapsed time: 0.1162722110748291\n",
            "loss: 8.904549598693848      Elapsed time: 0.11585116386413574\n",
            "loss: 8.984381675720215      Elapsed time: 0.11588239669799805\n",
            "loss: 8.963656425476074      Elapsed time: 0.11464834213256836\n",
            "loss: 9.069906234741211      Elapsed time: 0.11578798294067383\n",
            "loss: 8.942991256713867      Elapsed time: 0.11635899543762207\n",
            "loss: 8.8988037109375      Elapsed time: 0.11616873741149902\n",
            "loss: 8.950746536254883      Elapsed time: 0.11619853973388672\n",
            "loss: 8.830087661743164      Elapsed time: 0.11612796783447266\n",
            "loss: 9.132087707519531      Elapsed time: 0.11671590805053711\n",
            "loss: 9.084320068359375      Elapsed time: 0.11523103713989258\n",
            "loss: 8.902616500854492      Elapsed time: 0.11592602729797363\n",
            "loss: 8.789034843444824      Elapsed time: 0.11595964431762695\n",
            "loss: 8.837895393371582      Elapsed time: 0.11594843864440918\n",
            "loss: 8.854093551635742      Elapsed time: 0.11602544784545898\n",
            "loss: 8.942654609680176      Elapsed time: 0.11614680290222168\n",
            "loss: 8.911947250366211      Elapsed time: 0.11646747589111328\n",
            "loss: 8.853843688964844      Elapsed time: 0.11753249168395996\n",
            "loss: 9.027618408203125      Elapsed time: 0.1168673038482666\n",
            "loss: 8.834596633911133      Elapsed time: 0.11621642112731934\n",
            "loss: 8.729292869567871      Elapsed time: 0.11581587791442871\n",
            "loss: 8.93031120300293      Elapsed time: 0.1165776252746582\n",
            "loss: 8.990778923034668      Elapsed time: 0.11618185043334961\n",
            "loss: 9.003463745117188      Elapsed time: 0.11593770980834961\n",
            "loss: 8.999170303344727      Elapsed time: 0.11655592918395996\n",
            "loss: 8.834476470947266      Elapsed time: 0.11609792709350586\n",
            "loss: 8.79157829284668      Elapsed time: 0.11620831489562988\n",
            "loss: 8.902961730957031      Elapsed time: 0.11601948738098145\n",
            "loss: 8.872367858886719      Elapsed time: 0.11607861518859863\n",
            "loss: 8.961069107055664      Elapsed time: 0.11556625366210938\n",
            "loss: 8.822332382202148      Elapsed time: 0.11677050590515137\n",
            "loss: 8.973045349121094      Elapsed time: 0.11679434776306152\n",
            "loss: 8.90413761138916      Elapsed time: 0.11576294898986816\n",
            "loss: 8.841479301452637      Elapsed time: 0.11643099784851074\n",
            "loss: 8.841723442077637      Elapsed time: 0.11643052101135254\n",
            "loss: 8.936408996582031      Elapsed time: 0.11591601371765137\n",
            "loss: 8.84253215789795      Elapsed time: 0.11634206771850586\n",
            "loss: 8.651827812194824      Elapsed time: 0.11630439758300781\n",
            "loss: 8.98575210571289      Elapsed time: 0.11645817756652832\n",
            "loss: 8.957267761230469      Elapsed time: 0.11736655235290527\n",
            "loss: 8.646049499511719      Elapsed time: 0.11645126342773438\n",
            "loss: 8.895625114440918      Elapsed time: 0.1164100170135498\n",
            "loss: 8.876782417297363      Elapsed time: 0.11667847633361816\n",
            "loss: 8.897276878356934      Elapsed time: 0.11599993705749512\n",
            "loss: 8.85886001586914      Elapsed time: 0.11567354202270508\n",
            "loss: 8.847864151000977      Elapsed time: 0.11643862724304199\n",
            "loss: 8.782052040100098      Elapsed time: 0.11686468124389648\n",
            "loss: 8.932709693908691      Elapsed time: 0.11597418785095215\n",
            "loss: 8.712854385375977      Elapsed time: 0.11567211151123047\n",
            "loss: 8.843827247619629      Elapsed time: 0.11572122573852539\n",
            "loss: 8.878875732421875      Elapsed time: 0.11608266830444336\n",
            "loss: 8.820345878601074      Elapsed time: 0.11677026748657227\n",
            "loss: 8.69736385345459      Elapsed time: 0.11645007133483887\n",
            "loss: 8.771162986755371      Elapsed time: 0.11601638793945312\n",
            "loss: 8.736122131347656      Elapsed time: 0.11582708358764648\n",
            "loss: 8.799459457397461      Elapsed time: 0.11598849296569824\n",
            "loss: 8.768074035644531      Elapsed time: 0.11702394485473633\n",
            "loss: 8.850834846496582      Elapsed time: 0.11484432220458984\n",
            "loss: 8.748083114624023      Elapsed time: 0.11575889587402344\n",
            "loss: 8.94282054901123      Elapsed time: 0.11643075942993164\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}