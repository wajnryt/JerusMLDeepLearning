{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstanceClassifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajnryt/JerusMLDeepLearning2019/blob/Rachel/InstanceClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvEbIcsEMonV",
        "colab_type": "code",
        "outputId": "e26fd21d-845a-45d5-ca18-266c97ad17d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# mount data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQtzYuUM88R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataSet object\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "import numpy as np        \n",
        "\n",
        "\n",
        "class InstanceDataset(Dataset):\n",
        "          \n",
        "  \n",
        "    def __init__(self, basedir, transform=None):\n",
        "        super().__init__()\n",
        "        files = glob.glob(os.path.join(basedir ,'*','*','*.jpg'))\n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in files], \n",
        "                            columns=['instance_id', 'file_path'])\n",
        "\n",
        "        names  = np.unique(self.data['instance_id'])\n",
        "        \n",
        "        self.instanceDict = {str:index for index, str in enumerate(names)}\n",
        "          \n",
        "        self.data['instance_num'] = self.data['instance_id'].map(self.instanceDict)\n",
        "          \n",
        "    \n",
        "        \n",
        "    def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[1], f   #label is originaly a str\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = imageio.imread(dat['file_path'])\n",
        "      img = np.resize(img,(3,128,128))\n",
        "      return (img.astype(np.float32), dat['instance_num'])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "          \n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgdhhhjcwc1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "basedir = '/content/gdrive/My Drive/videos_2/yt_bb_detection_train'       \n",
        "        \n",
        "dataset = InstanceDataset(basedir)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nkVWqUqjEy",
        "colab_type": "code",
        "outputId": "42bb0052-f4d1-42fd-c3d8-4d434772745b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len((dataset.data['instance_num']))\n",
        "len(np.unique(dataset.data['instance_num']))\n",
        "dataset.data['instance_num'][200]\n",
        "max(np.unique(dataset.data['instance_num']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZBZOwY1Ra4",
        "colab_type": "code",
        "outputId": "d4960d9f-0e52-4ecb-e4e7-f9eee816981f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#Divide data to train/test\n",
        "dataset = InstanceDataset(basedir)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=test_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=False,\n",
        "                 num_workers=4)\n",
        "\n",
        "print('Train size: {}'.format(len(train_loader)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 1585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B45pmKnJqWZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0438d4ec-bd37-4d8f-8ed7-f62faac5e872"
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "import time \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter()    \n",
        "    \n",
        "num_final_in = model.fc.in_features\n",
        "\n",
        "NUM_CLASSES = len(np.unique(dataset.data['instance_num']))\n",
        "model.fc = nn.Linear(num_final_in, NUM_CLASSES)        \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "predicts=[]\n",
        "trainloss = []\n",
        "testloss = []\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "\n",
        "tf = time.time()\n",
        "\n",
        "for epoch in range(4):\n",
        "    # trainning\n",
        "    ave_loss = 0 \n",
        "    total_cnt = 0\n",
        "    correct_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "#         tf = time.time() \n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        \n",
        "        out = model(x)\n",
        "        loss = criterion(out, target) \n",
        "        \n",
        "        pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label[1] == target.data).sum()\n",
        "               \n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {}'.format(\n",
        "                epoch, batch_idx+1, ave_loss, float(correct_cnt)/total_cnt))    \n",
        "            \n",
        "        trainloss.append(loss.data)\n",
        "        \n",
        "        \n",
        "#         writer.add_text('train accuracy', 'train accuracy: ' + str(float(correct_cnt)/total_cnt))\n",
        "#         writer.add_text('train loss', 'train loss: ' + str(loss.data))\n",
        "        \n",
        "#         elapsed = time.time() - tf\n",
        "#         print(f'loss: {loss}      Elapsed time: {elapsed}')\n",
        "\n",
        "    \n",
        "     # testing    \n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        x = x.float()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)   \n",
        "        loss = criterion(out, target)\n",
        "        \n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        predicts.append(pred_label)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss,float(correct_cnt)/total_cnt))\n",
        "          \n",
        "        testloss.append(loss.data) \n",
        "#         writer.add_text('test accuracy', 'test accuracy: ' + str(float(correct_cnt)/total_cnt))\n",
        "#         writer.add_text('test loss', 'test loss: ' + str(loss.data))\n",
        "\n",
        "# pl.figure(1)\n",
        "# train_handle, = plt.plot(trainloss, 'ro-')\n",
        "# pl.show()\n",
        "    \n",
        "# pl.figure(1)\n",
        "# test_handle, = plt.plot(testloss, 'bo-')\n",
        "# pl.show()\n",
        "\n",
        "torch.save(model.state_dict(), 'yt_bb_detection_train/mymodel3')\n",
        "writer.close()\n",
        "\n",
        "elapsed = time.time() - tf\n",
        "print(f'Elapsed time: {elapsed}')\n",
        "\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==>>> epoch: 0, batch index: 100, train loss: 8.942140, acc: 0.0006\n",
            "==>>> epoch: 0, batch index: 200, train loss: 8.795403, acc: 0.0007\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}