{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataLoader_Youtube_bb_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajnryt/JerusMLDeepLearning2019/blob/David/Resnet18IntstanceClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmB8AVkNnO8q",
        "colab_type": "code",
        "outputId": "c103215e-09e6-4d00-e3e2-ec5843f36bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b1qft7BMhCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "basedir = '/content/gdrive/My Drive/videos_2/yt_bb_detection_train'\n",
        "#basedir = '/content/gdrive/My Drive/drive-download-20190703T155220Z-001'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfjUO4tdOSeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import datetime\n",
        "\n",
        "class InstanceDataset(Dataset):\n",
        "    def __init__(self, basedir, transform=None, force=False):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        cach_file = os.path.join(basedir, 'data.csv')\n",
        "        if not force and os.path.exists(cach_file):\n",
        "          self.data = pd.read_csv(cach_file)\n",
        "          print('load from csv')\n",
        "          return\n",
        "        print('start glob')\n",
        "        print(datetime.datetime.now())\n",
        "        files = glob.glob(os.path.join(basedir ,'*','*','*.jpg'))\n",
        "        print('finish glob')\n",
        "        print(datetime.datetime.now())\n",
        "        self.data = pd.DataFrame([self._split_file(f) for f in files], \n",
        "                            columns=['class_id',  'file_path'])\n",
        "        self.data.to_csv(cach_file)\n",
        "        print('finish sort')\n",
        "        \n",
        "    def _split_file(self, f):\n",
        "        parts = f.split(os.sep)[-3:-1]\n",
        "        return parts[0], f \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      dat = self.data.iloc[index]\n",
        "      img = Image.open(dat['file_path'])\n",
        "      if self.transform:\n",
        "        img = self.transform(img)\n",
        "    # return (img, dat['class_id'], dat['instance_id'])\n",
        "      return (img, int(dat['class_id']))        \n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9gm8gUJpdUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        #transforms.Pad(256),\n",
        "        #transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# dataset = InstanceDataset(basedir, data_transforms['train'], True)\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=3,\n",
        "#                                              shuffle=True, num_workers=4)\n",
        "\n",
        "#device = torch.device(\"cuda:0\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb4S9NmiQYco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
        "                                  else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
        "model.to(device)\n",
        "\n",
        "batch_size = 3\n",
        "dataset_train = InstanceDataset(basedir, data_transforms['train'], True)\n",
        "dataset_val = InstanceDataset(basedir, data_transforms['val'], True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#                  dataset=train_set,\n",
        "#                  batch_size=batch_size,\n",
        "#                  shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#                 dataset=test_set,\n",
        "#                 batch_size=batch_size,\n",
        "#                 shuffle=False)\n",
        "\n",
        "\n",
        "print ('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
        "print ('==>>> total testing batch number: {}'.format(len(test_loader)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2weIaXVOdfTv",
        "colab_type": "code",
        "outputId": "02ae7fc6-5b3a-452a-9eca-6220a3da445c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "# train_set = ImageFolder(root='/content/JerusML/My Drive/JerusML/Grayscale/', transform=ToTensor())\n",
        "# test_set = ImageFolder(root='/content/JerusML/My Drive/JerusML/GrayscaleTEST/', transform=ToTensor())\n",
        "\n",
        "\n",
        "## training\n",
        "model = models.resnet18()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    # trainning\n",
        "    ave_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        x, target = Variable(x), Variable(target)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss))\n",
        "    # testing\n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    total_cnt = 0\n",
        "    for batch_idx, (x, target) in enumerate(test_loader):\n",
        "        if use_cuda:\n",
        "            x, target = x.cuda(), target.cuda()\n",
        "        x, target = Variable(x, volatile=True), Variable(target, volatile=True)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        _, pred_label = torch.max(out.data, 1)\n",
        "        total_cnt += x.data.size()[0]\n",
        "        correct_cnt += (pred_label == target.data).sum()\n",
        "        # smooth average\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        \n",
        "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
        "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss,float(correct_cnt)/total_cnt))\n",
        "\n",
        "torch.save(model.state_dict(), \"resnet18s\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start glob\n",
            "2019-07-04 08:49:06.304776\n",
            "finish glob\n",
            "2019-07-04 08:58:56.347895\n",
            "finish sort\n",
            "start glob\n",
            "2019-07-04 08:58:57.800967\n",
            "finish glob\n",
            "2019-07-04 08:59:02.344304\n",
            "finish sort\n",
            "==>>> total trainning batch number: 33004\n",
            "==>>> total testing batch number: 33004\n",
            "==>>> epoch: 0, batch index: 100, train loss: 3.551440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsA4U5SYJLwU",
        "colab_type": "text"
      },
      "source": [
        "Using \n",
        "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQGnouoIIe1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hybMA8N9IlQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "a958152c-6b5c-4603-b157-aa9e5b32d6b3"
      },
      "source": [
        "epochs = 10\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 10\n",
        "train_losses, test_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        steps += 1\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logps = model.forward(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in test_loader:\n",
        "                    inputs, labels = inputs.to(device),                                     labels.to(device)\n",
        "                    logps = model.forward(inputs)\n",
        "                    batch_loss = criterion(logps, labels)\n",
        "                    test_loss += batch_loss.item()\n",
        "                    \n",
        "                    ps = torch.exp(logps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals =                        top_class == labels.view(*top_class.shape)\n",
        "                    accuracy +=                  torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "            train_losses.append(running_loss/len(train_loader))\n",
        "            test_losses.append(test_loss/len(test_loader))                    \n",
        "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                  f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
        "                  f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()\n",
        "torch.save(model, 'aerialmodel.pth')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10.. Train loss: -48.425.. Test loss: -56.317.. Test accuracy: 0.967\n",
            "Epoch 2/10.. Train loss: -55.721.. Test loss: -68.279.. Test accuracy: 0.900\n",
            "Epoch 3/10.. Train loss: -61.286.. Test loss: -69.569.. Test accuracy: 0.833\n",
            "Epoch 4/10.. Train loss: -69.840.. Test loss: -77.039.. Test accuracy: 0.833\n",
            "Epoch 5/10.. Train loss: -73.416.. Test loss: -87.482.. Test accuracy: 0.833\n",
            "Epoch 6/10.. Train loss: -81.879.. Test loss: -99.011.. Test accuracy: 0.833\n",
            "Epoch 7/10.. Train loss: -93.072.. Test loss: -105.631.. Test accuracy: 0.833\n",
            "Epoch 8/10.. Train loss: -97.685.. Test loss: -126.862.. Test accuracy: 0.833\n",
            "Epoch 9/10.. Train loss: -105.821.. Test loss: -126.379.. Test accuracy: 0.833\n",
            "Epoch 10/10.. Train loss: -113.414.. Test loss: -132.090.. Test accuracy: 0.833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK3VKhz1H7MI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c7add71b-cfbc-4939-cc9f-e8dec2e57c7a"
      },
      "source": [
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XdY1XX/x/Hnhy0iIKKiiOJWwIU4\nURFXjtQ0y5G5StNKW/7KrGzd3o3byhwNc5RpmmXm1lJx59YUXOAEFzhQFJH1+f3xJdNyIOt7Duf9\nuK5zKd+z3p0reZ3PVlprhBBC2DY7swsQQghhPgkDIYQQEgZCCCEkDIQQQiBhIIQQAgkDIYQQSBgI\nIYRAwkAIIQQSBkIIIQAHswvILm9vb+3v7292GUIIYTV27tx5XmtdMjuPtZow8Pf3Z8eOHWaXIYQQ\nVkMpdSK7j5VuIiGEEBIGQgghJAyEEEIgYSCEEAIJAyGEEEgYCCGEQMJACCEENhAGE1ZHs/XoBbPL\nEEIIi1aow+BKShqztpyg55Qt9J26lZ0nLpldkhAiGy5cuEDdunWpW7cuPj4++Pr63vw5NTU1W68x\ncOBADh06dM/HTJ48mdmzZ+dFyTRr1ow9e/bkyWuZQWmtza4hW0JCQnROViCnpGUwa8sJvlx7hAvX\nUmlZvSQvt61G7XKe+VClECKvvfPOO7i5uTFy5Mjbrmut0VpjZ2cZ32mbNWvGpEmTqFu3rtml3KSU\n2qm1DsnOYy3jU8xHLo72PN28EutfDee19jXYE5tIl0mbePq7HUSdvmx2eUKIBxATE0NAQABPPPEE\ngYGBnDlzhiFDhhASEkJgYCDvvffezcf+9U09PT0dT09PRo0aRZ06dWjSpAnx8fEAvPnmm4wfP/7m\n40eNGkXDhg2pXr06mzdvBuDatWs8+uijBAQE0KNHD0JCQu7bApg1axa1atUiKCiI0aNHA5Cens6T\nTz558/qECRMA+OyzzwgICKB27dr07ds3zz+z7LKavYlyq6izA8NaVqZv4/J8u+k432w4SqcJ5+gQ\n5MOLbapR3aeY2SUKYbHeXRzF/tNX8vQ1A8q683bnwAd+3sGDB5k5cyYhIcYX3g8//BAvLy/S09MJ\nDw+nR48eBAQE3Pacy5cvExYWxocffsjLL7/M9OnTGTVq1L9eW2vNtm3bWLRoEe+99x4rVqxg4sSJ\n+Pj4MH/+fP7880+Cg4PvWV9cXBxvvvkmO3bswMPDgzZt2rBkyRJKlizJ+fPn2bdvHwCJiYkAfPzx\nx5w4cQInJ6eb18xQ6FsG/1TMxZHhrauy4bVWjGhdlQ3R52n/+XqGz9lNTPxVs8sTQtxH5cqVbwYB\nwJw5cwgODiY4OJgDBw6wf//+fz2nSJEidOjQAYD69etz/PjxO7529+7d//WYjRs30qtXLwDq1KlD\nYOC9A2zr1q20atUKb29vHB0d6dOnD+vXr6dKlSocOnSIESNGsHLlSjw8PAAIDAykb9++zJ49G0dH\nxwf6LPKSzbQM/smjiCMvt63GoFB/pqw/yrebj7N072keqevLiNZV8fcuanaJQliMnHyDzy9Fi/79\nbzM6OprPP/+cbdu24enpSd++fUlJSfnXc5ycnG7+3d7envT09Du+trOz830fk1MlSpRg7969LF++\nnMmTJzN//nymTJnCypUrWbduHYsWLeK///0ve/fuxd7ePk/fOztsrmXwT56uTrzavgYbXg3n6eaV\nWBZ5htafruPVn/8k9mKy2eUJIe7hypUrFCtWDHd3d86cOcPKlSvz/D1CQ0OZN28eAPv27btjy+NW\njRo1IiIiggsXLpCens7cuXMJCwsjISEBrTWPPfYY7733Hrt27SIjI4O4uDhatWrFxx9/zPnz50lO\nNuf3js22DP6phJszozvW5OnmFfly7RFmbz3JL7tO8XgDP54Pr0JZzyJmlyiE+Ifg4GACAgKoUaMG\nFSpUIDQ0NM/fY/jw4fTr14+AgICbt7+6eO6kXLlyvP/++7Rs2RKtNZ07d6ZTp07s2rWLp556Cq01\nSik++ugj0tPT6dOnD0lJSWRmZjJy5EiKFTNn/LLQTy3NqTOXr/NFxBHmbj+JQtG7oR/PhlehtLtL\ngdUghDBfeno66enpuLi4EB0dTbt27YiOjsbBwfK/Sz/I1FLL/68xSRmPIrz/SBDPhFVickQMs7ee\nZO72WPo2rsDQsMqULOZsdolCiAJw9epVWrduTXp6Olprvv76a6sIggclLYNsOnkhmQlrovllVxzO\nDvb0a1qBZ1pUxquo0/2fLIQQJniQloGEwQM6mnCVCaujWfjnaVwd7RkYWpHBzSvh4WrelDAhhLgT\nCYMCEH0uifGrolm67wzFnB14qnlFBjWriLuLhIIQwjJIGBSgA2euMH7VYVZGncOjiCNDWlSif1N/\n3JwLX5+iEMK6SBiYIPLUZT77/TCrD8bjVdSJZ1pUol8Tf4o4FfziESGEANmozhRBvh5MG9CABc82\nJcjXgw+WH6T5xxFM23iMlLQMs8sTwqqEh4f/awHZ+PHjGTZs2D2f5+bmBsDp06fp0aPHHR/TsmVL\n7vfFcvz48bct/urYsWOe7Bv0zjvvMG7cuFy/Tn6QMMhj9coXZ+aghvw0tAnVSrvx/pL9hP0vgpl/\nHOd6qoSCENnRu3dv5s6de9u1uXPn0rt372w9v2zZsvz88885fv9/hsGyZcvw9Czc294X/jBI+/c+\nJQWhgb8XPwxuzJzBjSnv5cqYhVE0HLuKNxbsY29cItbSPSeEGXr06MHSpUtvHmRz/PhxTp8+TfPm\nzW/O+w8ODqZWrVosXLjwX88/fvw4QUFBAFy/fp1evXpRs2ZNunXrxvXr128+btiwYTe3v3777bcB\nmDBhAqdPnyY8PJzw8HAA/P39OX/+PACffvopQUFBBAUF3dz++vjx49SsWZPBgwcTGBhIu3btbnuf\nO9mzZw+NGzemdu3adOvWjUuXLt18/7+2tP5rg7x169bdPNynXr16JCUl5fizvZvCPcp54ypMbw81\nH4aw10CpAi+hSeUSzKvUhO3HLzF3+0nm74pj9taT1CzjTq8GfjxS11empQrLt3wUnN2Xt6/pUws6\nfHjHu7y8vGjYsCHLly+na9euzJ07l8cffxylFC4uLixYsAB3d3fOnz9P48aN6dKlC+ou/76//PJL\nXF1dOXDgAHv37r1tC+qxY8fi5eVFRkYGrVu3Zu/evYwYMYJPP/2UiIgIvL29b3utnTt3MmPGDLZu\n3YrWmkaNGhEWFkbx4sWJjo5mzpw5fPPNNzz++OPMnz//nucT9OvXj4kTJxIWFsaYMWN49913GT9+\nPB9++CHHjh3D2dn5ZtfUuHHjmDx5MqGhoVy9ehUXl7zfCaFwtwzsHaFMbVj7Afw8CNLundT5RSlF\nw4pefPp4XbaObsP7jwThYKd4e1EUDf67ihfm7mZzzHkyM6W1IMRfbu0qurWLSGvN6NGjqV27Nm3a\ntOHUqVOcO3furq+zfv36m7+Ua9euTe3atW/eN2/ePIKDg6lXrx5RUVH33YRu48aNdOvWjaJFi+Lm\n5kb37t3ZsGEDABUrVrx5ytm9tskG43yFxMREwsLCAOjfvz/r16+/WeMTTzzBrFmzbq50Dg0N5eWX\nX2bChAkkJibmywrowt0ycHCGrpOhZHX4/W24dBx6/QDuZUwryaOII082rsCTjSsQdfoy87bHsmD3\nKRbuOU15L1ceDylHj/p++HjIHkjCgtzlG3x+6tq1Ky+99BK7du0iOTmZ+vXrAzB79mwSEhLYuXMn\njo6O+Pv733Hb6vs5duwY48aNY/v27RQvXpwBAwbk6HX+8tf212BsgX2/bqK7Wbp0KevXr2fx4sWM\nHTuWffv2MWrUKDp16sSyZcsIDQ1l5cqV1KhRI8e13knhbhmA0TUU+oIRAgmH4JtWcNoyDq0OLOvB\nu12D2PZGGz7vVRdfzyKM++0wTT9czaBvt7My6ixpGZlmlymEKdzc3AgPD2fQoEG3DRxfvnyZUqVK\n4ejoSEREBCdOnLjn67Ro0YIffvgBgMjISPbu3QsY218XLVoUDw8Pzp07x/Lly28+p1ixYnfsl2/e\nvDm//vorycnJXLt2jQULFtC8efMH/m/z8PCgePHiN1sV33//PWFhYWRmZhIbG0t4eDgfffQRly9f\n5urVqxw5coRatWrx2muv0aBBAw4ePPjA73k/hbtlcKsaHeGp32BOL2McofvXENDV7KoA45zmrnV9\n6VrXlxMXrjFvRyw/7YhjzcF4vN2cebS+Lz1D/KhU0s3sUoUoUL1796Zbt263zSx64okn6Ny5M7Vq\n1SIkJOS+35CHDRvGwIEDqVmzJjVr1rzZwqhTpw716tWjRo0a+Pn53bb99ZAhQ2jfvj1ly5YlIiLi\n5vXg4GAGDBhAw4YNAXj66aepV6/ePbuE7ua7775j6NChJCcnU6lSJWbMmEFGRgZ9+/bl8uXLaK0Z\nMWIEnp6evPXWW0RERGBnZ0dgYODNU9vyku0tOrsaD3OfgLht0OpNaD7SlIHl+0nPyGTd4QTmbo9l\nzcF4MjI1Df296NnAj461yshiNiHEfVnMCmSl1HDgOSADWKq1fjXr+uvAU1nXR2it73s8UZ6uQE5L\ngcUjYO+PUOsx6DIJHC23jz4+KYX5O08xb0csx85fo5izA13qlqVnAz9q+XrcdRaFEMK2WUQYKKXC\ngTeATlrrG0qpUlrreKVUADAHaAiUBVYB1bTW91yRlefbUWgNGz+F1e+Bb4gxplCsdN69fj7QWrPt\n2EV+3B7LssgzpKRlUrOMOz1DyvFIPV88XWU7bSHE3ywlDOYBU7TWq/5x/XUArfUHWT+vBN7RWv9x\nr9fLt72J9i+CBc9AES/oPceYimoFLl9PY9Gfp5m3PZZ9py7j5GBH+0AfejXwo3GlEtjZSWtBCFtn\nKXsTVQOaK6W2KqXWKaUaZF33BWJveVxc1jVzBHSBQSsADdMfggNLTCvlQfw1RXXx8GYsHdGM3g38\nWHsonj5Tt9Jy3FomrYnm7GVzVl8LIaxPrloGSqlVgM8d7noDGAtEACOABsCPQCVgIrBFaz0r6zWm\nAcu11v/aSEQpNQQYAlC+fPn695tClitJZ2FuHzi1C9q8DaEvWuTA8r2kpGWwMuosc7fF8sfRC9gp\naFm9FD0b+NGqRikc7Qv/TGIhxN8spZtoBfCR1joi6+cjQGPgabCgbqJbpV2Hhc9B5Hyo0xs6f24s\nXLNCt05RjU+6IVNUhbBBlhIGQ4GyWusxSqlqwGqgPBAA/MDfA8irgaoFPoB8N1rD+v9BxFjwawQ9\nZ4Nbyfx/33zyzymqmVrTM8SPV9vXkPObhSjkLCUMnIDpQF0gFRiptV6Tdd8bwCAgHXhRa738ri+U\npcAPt4laAAuGQdGS0GculA4suPfOJ/FXUpiy/igzNh/HzdmB/3uoOr0blsdeBpuFKJQsIgzymikn\nnZ3aZYwj3EiCR6dC9bxf9WeGw+eSGLMwki1HLxLk6857XYMILl/c7LKEEHnMUmYTWT/fYBi8BkpU\ngTm9YdMEoxvJylUrXYw5gxszoXc9EpJu0P2LzfzfT39y/uoNs0sTQphEwuB+3MvCwOXGPka/vwUL\nn4f0VLOryjWlFF3qlGX1Ky15pkUlFuw+Ratxa/lu83HSZXM8IWyOhEF2OLlCjxkQNgr2zIKZXeHa\nebOryhNuzg683rEmK15sTq1yHry9KIrOkzax4/hFs0sTQhQgCYPssrOD8Nfh0WlwaqexFXb8AbOr\nyjNVShVj1lONmNwnmMTkVHp89Qcvz9tDQpJ0HQlhCyQMHlStHjBwGaSnwNS2EP272RXlGaUUnWqX\nYfUrYQxrWZnFf56m1bi1TN94TLqOhCjkJAxyolyIMbDs5Q8/PA5/TC4UA8t/cXVy4LX2NVjxYgvq\nlvfkvSX7eXjiRrYevWB2aUKIfCJhkFMe5WDQSqjRCVaOhsUvFIqB5VtVLunGzEEN+apvfZJS0uk5\nZQsvzt1N/BXZ80iIwkbCIDecisJjM6H5K7DrO5jVHZIL18CrUor2QT6sejmM4a2qsGzfWVp9so6p\nG47KkZxCFCISBrllZwetx0C3KRC71RhYTjhkdlV5roiTPa+0q85vL7WggX9x/rP0AJ0mbOCPI9J1\nJERhIGGQV+r0hAFLIfUqTG0DMavu/xwr5O9dlOkDGvBNvxCSUzPo/c0Whs/ZLdtlC2HlJAzykl9D\nY2DZszzMfgy2fl2oBpb/opSibUBpVr0cxgutq7Iy6iytP1nL1+uOkJouXUdCWCMJg7zmWd4YWK7W\nHpa/Cktfhow0s6vKFy6O9rzUthqrXgqjSeUSfLD8IB0+X8+mmMKxIE8IWyJhkB+c3aDnLAh9AXZM\nL5QDy7cqX8KVqf0bMH1ACGkZmiembuW52bs4nXjd7NKEENkku5bmtz0/wKIR4OkHgd2gaCnjfISi\npcCtlLFFdpHiVneq2t2kpGUwZf1RJkfEYKcUz7eqwtPNK+LsYG92aULYHNnC2tKc+AMWPQ8Xj8Gd\nzvCxczRC4Z8h4Vbq3+FRxMuYwWThYi8m8/6S/fy2/xyVvIvyTpdAWlSz3kOChLBGEgaWKjMTrl+C\na/FwNR6uJWT9GQ9XE/6+/td9mXcYa1D2UNT773C4V3C4lgA7c7+Rrz0UzzuLojh+IZn2gT68+XBN\nyhV3NbUmIWyFhEFhoHVWcNwlMG5ez/oz4w4byik7IxBuDQlPP6g/wBjoLiA30jOYuuEYE9dEA/Bc\nyyoMblEJF0fpOhIiP0kY2Bqt4caVu4TFrSFyDq6cNkIiZJCxctqtVIGVeSrxOv9Zsp/lkWcp7e5M\n86olaVypBI0reUlrQYh8IGEg7i4xFtZ/DLtng4MzNBoKoSOMQewCsiE6gVlbTrDt2EUuJRtdYeWK\nF8kKBgkHIfKKhIG4vwtHIOK/EPkzOHtA6HBoNMyYFltAMjM1h+OT2HLkAluOXmTrsQsSDkLkIQkD\nkX1nIyFiLBxaBq7eRtdRyCBwdCnwUrIbDo0qeuHnJeEgxP1IGIgHF7sd1rwPx9aBuy+EvQZ1+4C9\no2klSTgIkTsSBiLnjq4zQiFuO3hVgvA3ILC7RaxtuFc4+HoWudml1LhSCQkHIZAwELmlNRxeAavf\nh/goKBUIrd6E6h0saqW0hIMQ9yZhIPJGZiZE/WIMNF88Ar4h0PotqNTS7MruKDNTEx1/lS1HL9y8\nSTgIWyZhIPJWRjr8+QOs/QiuxEHFFtBqDPg1MLuye8pOOLQP8qFNzVIoC2rxCJFXJAxE/khLgZ0z\nYMMnxqK2ah2M7iOfILMry5Z/hsPWYxe5eC2V0ColeKdzIFVLFzO7RCHylISByF83rsLWr2DTBLhx\nGYIehZajwbuK2ZU9kPSMTH7YdpJxKw+RnJpB/6b+vNCmKu4u5s2gEiIvSRiIgnH9EmyeCFu+hPQb\nxlTUsNeM/Y+syIWrNxj32yHmbo+lRFFnRnWoQfd6vtjZSdeRsG4SBqJgXY2HDZ/CjmnGzyFPQfOX\nC3Tfo7ywNy6RMQuj2BObSHB5T97tEkStch5mlyVEjkkYCHMkxsK6j4wDfRycofEwaDq8QPc9yq3M\nTM38XXF8tOIgF66l0qtBef7voep4FXUyuzQhHpiEgTDX+RhY+1+InA8uHtB0hLEhXgHue5RbV1LS\nGP97NN/9cRw3ZwdeaVeNPg3L42Bv/uI7IbJLwkBYhrP7YM1YOLzcOICn+StQf6Ap+x7l1OFzSby9\nMIo/jl6gZhl33u0SSMOKXmaXJUS2SBgIyxK7HVa/C8c3gHs5aPka1OkD9g5mV5YtWmuW7TvL2KX7\nOX05ha51yzK6Y01Ku1tPqAnbJGEgLNPRtcYWF6d2gHc16P4NlK1rdlXZlpyazhcRR5iy/iiO9orh\nrasyKLQiTg7SdSQsk4SBsFxaG9tlLx1pLFxr+54x0GxFK4BPXLjG+0v2s+pAPJW8i/J2l0DCqpU0\nuywh/uVBwkC+0oiCpRTU6ATDNkHVtrDydfjhcbh23uzKsq1CiaJM7d+AGQMakKk1/advY/DMHcRe\nTDa7NCFyTFoGwjxaw7Zv4Lc3jemn3adApTCzq3ogN9IzmLbxGBNXx5ChNUPDKjMsrDJFnOzNLk0I\ny2gZKKXqKqW2KKX2KKV2KKUaZl1XSqkJSqkYpdRepVRwftUgLJxS0GgIDF4NLu4wsyusehcy0syu\nLNucHex5tmUV1owM46FAHyasjqbNp+tYEXkGa/miJQTkbzfRx8C7Wuu6wJisnwE6AFWzbkOAL/Ox\nBmENfGrBkLVQry9s/BRmdIBLJ8yu6oGU8SjCxN71mDO4MW7ODgydtYsnp20jJj7J7NKEyJb8DAMN\nuGf93QM4nfX3rsBMbdgCeCqlyuRjHcIaOBWFrpOgx3RIOARfNYeoBWZX9cCaVC7B0hHNeKdzAH/G\nJdJ+/AbGLt1PUor1tHaEbcrPMHgR+J9SKhYYB7yedd0XiL3lcXFZ14QwdkB9Zj14V4WfBsCi4ZBq\nXQOzDvZ2DAitSMTIljwaXI6pG4/R6pN1zN8ZR2amdB0Jy5SrMFBKrVJKRd7h1hUYBryktfYDXgKm\n5eD1h2SNN+xISEjITanCmnhVhEErIPRF2DUTprSEc1FmV/XAvN2c+ahHbRY8G0pZDxde+elPHvv6\nDyJPXTa7NCH+Jd9mEymlLgOeWmutjGOkLmut3ZVSXwNrtdZzsh53CGiptT5zr9eT2UQ26sgaWDAU\nrifCQ2OhwdNWtSbhL5mZmp93GhvgXUxOpXfD8vxfu+oUlw3wRD6yiNlEGGMEf80TbAVEZ/19EdAv\na1ZRY4yQuGcQCBtWuRUM3QQVm8OykfBjX0i+aHZVD8zOTvF4Az/WjGxJ/yb+/Lg9lvBP1vL9lhNk\nSNeRsAD52TJoBnwOOAApwLNa651ZrYRJQHsgGRiotb7vV35pGdi4zEzY8gWsesc4J+HRqVChqdlV\n5djBs1d4Z1EUW45eJKCMOx90r0UdP0+zyxKFjGxHIQqvU7tg/lNw6bhxqlqL/wM761zgpbVmyd4z\njF16gAvXbjCmcyB9G5VHWWE3mLBMltJNJETe8w02ZhvVegzWfgDfdYbLcWZXlSNKKTrXKcuKF5vT\ntLI3b/0aycif9pKSlmF2acIGSRgI6+NczNi6otvXcHoPfNUMDi41u6oc83R1YsaABrzQuirzd8XR\n/YvNnLxgXdNphfWTMBDWq04vGLoBPMvD3D7GTqhpKWZXlSN2doqX2lZj+oAQ4i4l8/DEDUQcjDe7\nLGFDJAyEdStRGZ76HRo/B9u/gamtjRXMVqpVjdIsHt4M3+KuDPpuO5/9flgWqokCIWEgrJ+DM7T/\nL/T5CZLOGIvUds00dkW1QhVKFOWXYU3pVteXz1dH89R320lMTjW7LFHISRiIwqNaO2NNQrkQYxuL\nnwdBinWu9i3iZM8nj9fh/UeC2Bhzns6TNsrKZZGvJAxE4eJeBp78FVqPgf0LjcHl2O1mV5UjSime\nbFyBH59pQlq65tEvN/PzTuucOSUsn4SBKHzs7KH5K8b+RhqY/hBs+NRYuGaFgssXZ8mIZtQr78nI\nn/7kzV/3cSNdpp+KvCVhIAovv4bGbKOALrD6Xfj+EUg6a3ZVOeLt5syspxrxTItKzNpykp5fb+HM\n5etmlyUKEQkDUbgV8YQeM6DzBIjdBl+GQvTvZleVIw72drzesSZfPhFM9LkkHp6wkc0x1nN2tLBs\nEgai8FMK6veHZ9ZBMR+Y3QNWjIb0G2ZXliMdapVh4fPNKF7Uib7TtvLVuiNyxKbINQkDYTtKVoen\nV0ODwbBlMkxrCxeOmF1VjlQp5cavz4XSIagMHy4/yLBZu+Q0NZErEgbCtji6QKdx0HO2cc7ylJZw\naIXZVeWIm7MDk/rU442ONfn9wDm6Tt5E9Dk5c1nkjISBsE01H4ahG41T1eb0gnX/s8rZRkopBreo\nxKynGnHlehpdJ29iyd7T93+iEP8gYSBsl6cfDFxh7IAa8R/4qR/cuGp2VTnSpHIJlgxvTg2fYjz/\nw27+s2Q/6RnWF27CPBIGwrY5uRo7oLYba+x8Oq0tXDxqdlU54uPhwtwhTejfpAJTNx7jialbSUiy\nzkFyUfAkDIRQCpo+D31/ydrbKBxiVptdVY44OdjxbtcgPutZhz/jEnl44gZ2nrC+Y0JFwZMwEOIv\nlcNhcAS4+xrTTzdNsNrN7rrVK8cvw0JxdrCn59db+G7zcZl+Ku5JwkCIW3lVhKd/h5pd4Pe3YP7T\nkGqdB80ElHVn8fPNCKtWkrcXRfHSj3tITk03uyxhoSQMhPgnp6Lw2LfGZneR82F6O0g8aXZVOeLh\n6sg3/UJ4pW01Fv55mu5fbOb4+WtmlyUskISBEHeilLHZXZ95cOmksR7h2Aazq8oROzvF8NZV+XZg\nQ85eSaHzpI2s2n/O7LKEhZEwEOJeqrWDwWvA1RtmdoWtX1vtOEJYtZIsfr4ZFUq48vTMHXzy2yEy\n5BQ1kUXCQIj78a4CT6+Cag/B8ldh4XNWe9ayn5crPw9tymP1yzFxTQwDZmzj0jU5RU1IGAiRPS7u\nxhYWYaNgz2z4tiNcsc6Vvi6O9nzcozYfdK/F1qMXeXjiRvbFySlqtk7CQIjssrOD8NeNUEg4BF+H\nwcktZleVI0opejcsz09Dm6C15tGvNjNve6zZZQkTSRgI8aBqPmx0Gzm7wbcPw44ZZleUY3X8PFky\nojkN/b14df5eOn6+gYmro4mJlw3vbI2yloUoISEheseOHWaXIcTfrl8y1iHErIL6A6HDx+DgZHZV\nOZKRqZm15QQL95xi18lEACqXLEqHoDK0D/IhsKw7SimTqxQPSim1U2sdkq3HShgIkQuZGbDmfdj4\nGfg1hsdnQrHSZleVK+eupLAy6iwrIs+y5egFMjWUK16E9oE+dKjlQz2/4tjZSTBYAwkDIQpa5C/G\nLCMXT+g1C3zrm11Rnrh4LZVV+8+xPPIMG2POk5ahKVXMmYcCfWgf5EOjil442Etvs6WSMBDCDGf3\nwdw+kHQOOo+Hun3MrihPXUlJI+JgPMv3nWXt4XhS0jIp7upI24DStA/yIbSKN84O9maXKW4hYSCE\nWZIvwk8D4Ng6aDQU2v0H7B2W6PH5AAATHUlEQVTNrirPXU/NYN3heFZEnmX1gXiSbqTj5uxAqxql\n6BDkQ1j1krg6OZhdps2TMBDCTBnp8PsY45xl/+bGPkdFvc2uKt+kpmey6ch5Vkae5bf957h4LRVn\nBzvCqpWkQy0fWtUojUeRwheI1kDCQAhL8OdcWDQC3EpDr9lQprbZFeW79IxMth2/yMrIs6yIOsu5\nKzdwtFc0rexNhyAf2gaUpoSbs9ll2gwJAyEsxald8GNfo/uo6ySo1cPsigpMZqZmT1wiKyLPsjzy\nDLEXr2OnoGFFL9oH+vBQkA9lPIqYXWahJmEghCW5Gg/z+sPJzRD6ArR+G+xsa6BVa83+M1dYGXmW\n5ZFniY43zpqu6+dJhyBjZlKFEkVNrrLwkTAQwtKkp8LK0bD9G6jcCh6dBq5eZldlmpj4q6yMMloM\nkaeuAFCzjDvtA33oWMuHqqWLmVxh4SBhIISl2vkdLBtpHK3Z6wcoHWB2RaaLvZh8c5HbzpOX0Boe\nCizN6I41pbWQSxIGQliy2G3w45NwIwm6fQUBXcyuyGLEX0lh7vZYvlp3hPQMzYBQf55vVQV3F5mN\nlBMPEga5WjqolHpMKRWllMpUSoX8477XlVIxSqlDSqmHbrnePutajFJqVG7eXwir5NcQhqw1WgXz\nnoQ1YyEz0+yqLEIpdxdGtK7K2pEt6Vq3LN9sOEr4/9Yya8sJ0jPkM8pPuV1HHgl0B9bfelEpFQD0\nAgKB9sAXSil7pZQ9MBnoAAQAvbMeK4RtcS8DA5ZCvb6w/mOY1gYOLJZQyFLK3YX/PVaHxc83o3Ip\nN978NZJOEzayITrB7NIKrVyFgdb6gNb60B3u6grM1Vrf0FofA2KAhlm3GK31Ua11KjA367FC2B4H\nZ+gyCR75yph6+mNfmNzAGFew0pPU8lqQrwc/DmnMl08Ek5yWzpPTtvHUt9s5knDV7NIKnfzaYcoX\nuPWkjLisa3e7LoRtUgrq9obhO42Vyk5usHgEfF4bNnwK1xPNrtB0Sik61CrDqpfDeL1DDbYeu8hD\nn63n3cVRJCbLkZ155b5hoJRapZSKvMMt37/RK6WGKKV2KKV2JCRI81AUYnb2ENjNGEvotwhKB8Hq\nd+GzIPjtTas9YjMvOTvY80xYZSJGtuSxED++23ycluPW8u2mY6TJeEKu5clsIqXUWmCk1npH1s+v\nA2itP8j6eSXwTtbD39FaP3Snx92LzCYSNufMXtg8wdgeW9lB7Z4QOgJKVje7Motw8OwV3l+yn00x\nF6hcsihvdgqgZfWScgjPLQpsNtE9LAJ6KaWclVIVgarANmA7UFUpVVEp5YQxyLwon2oQwrqVqQ2P\nToURuyBkEETOh8kNYU5vqz17OS/V8HFn1lONmNovhEwNA7/dTv8Z2zl8To7szIlctQyUUt2AiUBJ\nIBHYc8u3/jeAQUA68KLWennW9Y7AeMAemK61Hpud95KWgbB51y4YK5i3fg3XL4JfIwh9Eaq1Bzvb\nPmAmNT2T77ec4PNVh7mWmkGfhuV5qW01vIpa5zGkeUUWnQlRmKVeg92z4Y+JkHgSvKsb3Ue1HjNm\nKNmwS9dSGb/qMLO2nsTVyZ4XWlelXxN/nBxsMywlDISwBRnpsP9X2DTeOGWtWBlo/CzUHwAu7mZX\nZ6roc0mMXXaAtYcS8C/hyuiONWkbUNrmxhMkDISwJVrDkTWw6XPjhDVnd2OMofEwKOZjdnWmWnso\nnv8sPUBM/FWaVCrBWw8HEFDWdoJSwkAIW3V6txEK+xeCnQPU6QVNR4B3VbMrM016RiY/bDvJZ78f\nJvF6Gj1D/HilXXVKFiv8XWoSBkLYuotHYfMk2DMb0m9AjU7GYLNfA7MrM83l5DQmrInmu83HcXG0\n59nwygwKrYiLY+E9W0LCQAhhuJoA26YYt5REqBBqHLBTtZ2x+tkGHU24yn+XHWTVgXOUK16E1zvU\npGMtn0I5niBhIIS43Y2rsPt7o7VwJQ5KBRjdR0GPgoNtTr/cFHOe95fs5+DZJBr4F2fMw4HUKudh\ndll5SsJACHFnGWnGiuZNn0N8lHHITpPnILgfONve6WIZmZp5O2L55LdDnL+ayqPB5Xi1fXVKu7uY\nXVqekDAQQtyb1hCzygiF4xvAxQMaDIZmL9pkKCSlpDE54gjTNx7D3k4xrGVlBjevRBEn6x5PkDAQ\nQmRf3A4jFA4shpqd4fGZNjuecPJCMh+uOMCyfWcp4+HCS22q0T3YFwd761y0Zgl7EwkhrEW5EOj5\nPbR5Gw4sMvZAslHlS7jyxRP1+XFIY0q5u/Dq/L08NH49KyLPYC1fnHNKwkAIYWg6Aso1gGUjIemc\n2dWYqlGlEvz6bFO+6lsfpRRDZ+3ikS82sznmvNml5RsJAyGEwc4eHvkS0q7D4heMcQUbppSifZAP\nK19swf961CbhSgp9pm6l79St7I0rfIcOSRgIIf7mXRVaj4HDy+HPuWZXYxHs7RSPhfixZmRL3no4\ngP1nrtBl0iaenb2zUB2/KQPIQojbZWbAt53g3H549g/wkJNpb5WUksbUDceYuuEoKemZPFa/HC+0\nqUoZjyJml/YvMoAshMg5O3t45AvITDPOY7aSL4wFpZiLIy+1rcb6V8Pp38SfX3adIux/axm7dD+X\nrlnvmcwSBkKIf/OqBG3fM9Yi7JppdjUWqYSbM2M6B7BmZBhd6pRl2sZjtPg4gomro7l2I93s8h6Y\ndBMJIe4sMxNmdoHTe+DZzeBZ3uyKLFr0uSTG/XaIlVHn8HZz4vnwKvRuVB5nB/MWrkk3kRAi9+zs\noOtkQMPC54xwEHdVtXQxvn4yhF+ebUqVUm68s3g/rT9Zxy+74sjItPwv3RIGQoi7K14B2v0Hjq2H\nHdPMrsYqBJcvzpzBjZk5qCGero68PO9POn6+gd/3n7PohWsSBkKIe6s/ACq3ht/HwMVjZldjFZRS\ntKhWkkXPNWNSn3qkZmQyeOYOHv1yM1uPXjC7vDuSMBBC3JtS0GUi2DlKd9EDsrNTPFy7LL+91IIP\nutfiVOJ1ek7ZwoAZ24g6fdns8m4jYSCEuD8PX2j/AZzYBFu/Mrsaq+Nob0fvhuVZ93/hjO5Yg90n\nE+k0YSPD5+zm+PlrZpcHyGwiIUR2aQ1zesHRtTB0E3hXMbsiq3X5ehrfrD/KtI3HSMvIpGcDP0a0\nrprn5yjIFtZCiPyRdBYmNwLvajBohbFATeRYfFIKk9bE8MPWkzjYKwaGVmRoi8p4uDrmyevL1FIh\nRP4o5gMdx0HcNvhjktnVWL1SxVx4r2sQa15pSYegMny17gjNP17DF2tjuJ6aUaC1SMtACPFgtIYf\n+0L07/DMeihVw+yKCo0DZ64wbuUhVh+Mp1QxZ4a3rkqvBn445vBwHWkZCCHyj1Lw8HhwdoNfh0KG\n9W29YKlqlnFn2oAG/Dy0CRVKuPLWr5E89Nn6AmklSBgIIR6cW0no9Amc3g2bPjO7mkInxN+Lec80\nYcaABnSt61sgZzFLGAghciawGwR2h7Ufwdl9ZldT6CilCK9RihfaVC2Q95MwEELkXKdPoEhx+HUY\npFvv9s1CwkAIkRuuXtB5vNEy2DDO7GpELkgYCCFyp0YnqN0L1o8ztrsWVknCQAiRex0+BLdSsGAo\npN8wuxqRAxIGQojcK1Lc2Mwu4QCs/cDsakQOSBgIIfJG1bZQ70nY9DnEyQJRayNhIITIOw+NhWJl\nje6itOtmVyMegISBECLvuHhA10lwIRrW/MfsasQDkDAQQuStyuEQMgj+mAwn/jC7GpFNEgZCiLzX\n9n3wLG8sRku1jMNbxL3lKgyUUo8ppaKUUplKqZBbrrdVSu1USu3L+rPVLffVz7oeo5SaoJRSualB\nCGGBnN3gkS/g0jFY9a7Z1YhsyG3LIBLoDqz/x/XzQGetdS2gP/D9Lfd9CQwGqmbd2ueyBiGEJfJv\nBo2Gwrav4dg/f0UIS5OrMNBaH9BaH7rD9d1a69NZP0YBRZRSzkqpMoC71nqLNg5SmAk8kpsahBAW\nrPXb4FUJFj4HN5LMrkbcQ0GMGTwK7NJa3wB8gbhb7ovLunZHSqkhSqkdSqkdCQkJ+VymECLPObnC\nI19CYiz89pbZ1Yh7uG8YKKVWKaUi73Drmo3nBgIfAc/kpDit9RStdYjWOqRkyZI5eQkhhNnKN4am\nz8POGRCz2uxqxF043O8BWus2OXlhpVQ5YAHQT2t9JOvyKaDcLQ8rl3VNCFGYhb8Bh1fCouHw7B/G\negRhUfKlm0gp5QksBUZprTf9dV1rfQa4opRqnDWLqB+wMD9qEEJYEMci8MhXkHQGVow2uxpxB7md\nWtpNKRUHNAGWKqVWZt31PFAFGKOU2pN1K5V137PAVCAGOAIsz00NQggrUa4+NHsJ9swyWgnCoihj\nUo/lCwkJ0Tt2yOZXQli19BswJRySLxjdRa5eZldUqCmldmqtQ+7/SFmBLIQoSA7OxmK05POw/DWz\nqxG3kDAQQhSssnWh+UjYNw8OLDa7GpFFwkAIUfBajASf2rDkJbh2wexqBBIGQggz2DtCt6/geiIs\ne8XsagQSBkIIs5QOhJajIGoBRP5idjU2T8JACGGe0BehbDAsfQWuxptdjU2TMBBCmMfeweguSr0G\ni18EK5nqXhhJGAghzFWyOrR6Ew4thb3zzK7GZt13byIhhMh3TZ6Dg0tg+f9BxRbgXib/3ktrSE8x\nbmkpkH7dWAyXlvVn+nXjuqML+Dc3BrttgISBEMJ8dvbGVtdfhhqb2YW9+u9fzjd/gd9y/eYv8bv9\ncr/D9fSU7NdVtBTU6wvB/cCrYv7991sA2Y5CCGE5tnwFK7K5MtnO0dgAz8HFuDm6GCucHYpk/f2v\n60Xuf93B+d+vdeUM7P4eDq8AnQmVwqH+AKjRyWpaCw+yHYWEgRDCcmgNJ7cYA8qOLnf/Re3gYrQm\nCsLlU7BnNuyaCZdjs1oLT2S1FioVTA05JGEghBB5LTPDOJxn57dZrYUMqNTSaC1U7wQOTubWdwcP\nEgYyZiCEENlhZw/V2hm3K6dh92zY9R38NABcvbNaC/2hRGWzK80RaRkIIUROZWbAkQjjSM9Dy43W\nQsUWUH8g1HjY9NaCtAyEEKIg2NlD1TbG7coZ4+CenTPh54FGa6FuH6MbyQpaC9IyEEKIvJSZCUfX\nGGMLB5cZrQX/5kYo1OxsDIgXEGkZCCGEWezsoEob45Z0FnbPMsYW5j8FRbyyWgsDwbuK2ZXeRloG\nQgiR3zIz4WiE0Vo4tAwy0wuktSAtAyGEsCR2dlCltXFLOpe1buEfrYXg/lCymmklSstACCHMkJkJ\nx9ZmjS0sNVoLFUKNLqSanY3FdbkkLQMhhLB0dnZQuZVxuxpvtBZ2fgu/PA1FikOdPlC/v7GrawGQ\nloEQQliKzEw4ti6rtbDk79bCkwtyNK4gLQMhhLBGdnZQOdy4XY2HPT/AxSMFMh1VwkAIISyRWylo\n9mKBvZ2cdCaEEELCQAghhISBEEIIJAyEEEIgYSCEEAIJAyGEEEgYCCGEQMJACCEEVrQdhVIqATiR\nw6d7A+fzsBxrJp/F7eTzuJ18Hn8rDJ9FBa11yew80GrCIDeUUjuyuz9HYSefxe3k87idfB5/s7XP\nQrqJhBBCSBgIIYSwnTCYYnYBFkQ+i9vJ53E7+Tz+ZlOfhU2MGQghhLg3W2kZCCGEuIdCHQZKqfZK\nqUNKqRil1Ciz6zGTUspPKRWhlNqvlIpSSr1gdk1mU0rZK6V2K6WWmF2L2ZRSnkqpn5VSB5VSB5RS\nTcyuyUxKqZey/p1EKqXmKKVyfyCxhSu0YaCUsgcmAx2AAKC3UirA3KpMlQ68orUOABoDz9n45wHw\nAnDA7CIsxOfACq11DaAONvy5KKV8gRFAiNY6CLAHeplbVf4rtGEANARitNZHtdapwFygq8k1mUZr\nfUZrvSvr70kY/9h9za3KPEqpckAnYKrZtZhNKeUBtACmAWitU7XWieZWZToHoIhSygFwBU6bXE++\nK8xh4AvE3vJzHDb8y+9WSil/oB6w1dxKTDUeeBXINLsQC1ARSABmZHWbTVVKFTW7KLNorU8B44CT\nwBngstb6N3Oryn+FOQzEHSil3ID5wIta6ytm12MGpdTDQLzWeqfZtVgIByAY+FJrXQ+4BtjsGJtS\nqjhGL0JFoCxQVCnV19yq8l9hDoNTgN8tP5fLumazlFKOGEEwW2v9i9n1mCgU6KKUOo7RfdhKKTXL\n3JJMFQfEaa3/ain+jBEOtqoNcExrnaC1TgN+AZqaXFO+K8xhsB2oqpSqqJRywhgAWmRyTaZRSimM\nPuEDWutPza7HTFrr17XW5bTW/hj/X6zRWhf6b353o7U+C8QqpapnXWoN7DexJLOdBBorpVyz/t20\nxgYG1B3MLiC/aK3TlVLPAysxZgNM11pHmVyWmUKBJ4F9Sqk9WddGa62XmViTsBzDgdlZX5yOAgNN\nrsc0WuutSqmfgV0Ys/B2YwOrkWUFshBCiELdTSSEECKbJAyEEEJIGAghhJAwEEIIgYSBEEIIJAyE\nEEIgYSCEEAIJAyGEEMD/A9ktb+mIMSp3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Min9msub4ghn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(len(dataset.data['instance_id'].unique()), ' unique instances')\n",
        "# print(len(dataset.data['class_id'].unique()), ' classes')\n",
        "# print(len(dataset) , ' images')\n",
        "# import numpy as np\n",
        "\n",
        "# plt.figure(figsize=(15,15))\n",
        "# plt.hist(dataset.data['class_id'].values.astype(int), normed=True)\n",
        "# plt.title('images per class')\n",
        "# plt.xlabel('class id')\n",
        "\n",
        "# plt.figure(figsize=(15,15))\n",
        "# _, count = np.unique(dataset.data['instance_id'], return_counts=True)\n",
        "# plt.hist(count, normed=True)\n",
        "# plt.title('images per instance')\n",
        "# plt.xlabel('images per instance')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWZ2id8AeaYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "# #example. \n",
        "# # this is not how you're supposed to use dataset!\n",
        "# # you should axes it via torch DataLoader...\n",
        "# # never dirrectly call __getitem__, or access the members (dataset.data)\n",
        "\n",
        "# print(len(dataset))\n",
        "# im, class_id, instance_id = dataset.__getitem__(500)\n",
        "\n",
        "\n",
        "# #just for fun, lets find another one of this guy\n",
        "# thisguy = dataset.data[dataset.data['instance_id']==instance_id]\n",
        "# for ind, dat in thisguy.iterrows():\n",
        "#   plt.figure()\n",
        "#   plt.imshow(imageio.imread(dat['file_path']))\n",
        "#   plt.title(f'class id {dat[\"class_id\"]}, instance {dat[\"instance_id\"]}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQAy_C0wp_Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def imshow(inp, title=None):\n",
        "#     \"\"\"Imshow for Tensor.\"\"\"\n",
        "#     inp = inp.numpy().transpose((1, 2, 0))\n",
        "#     mean = np.array([0.485, 0.456, 0.406])\n",
        "#     std = np.array([0.229, 0.224, 0.225])\n",
        "#     inp = std * inp + mean\n",
        "#     inp = np.clip(inp, 0, 1)\n",
        "#     plt.imshow(inp)\n",
        "#     if title is not None:\n",
        "#         plt.title(title)\n",
        "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# # Get a batch of training data\n",
        "# inputs, classes, instance= next(iter(dataloader))\n",
        "\n",
        "# # Make a grid from batch\n",
        "# out = torchvision.utils.make_grid(inputs,3)\n",
        "# plt.figure(figsize=(15,15))\n",
        "# imshow(out)#, title=[class_names[x] for x in classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iEdbhgK1ZtL",
        "colab_type": "code",
        "outputId": "e7eee57c-ca51-4168-d89c-14da3aadb5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.autograd import Variable\n",
        "# import torchvision.datasets as dset\n",
        "# import torchvision.transforms as transforms\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torchvision.datasets import ImageFolder\n",
        "# from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "# batch_size = 3\n",
        "# dataset_train = InstanceDataset(basedir,dataset_train, True)\n",
        "# dataset_val = InstanceDataset(basedir, dataset_train, True)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
        "#                                              shuffle=True, num_workers=4)\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size,\n",
        "#                                              shuffle=True, num_workers=4)\n",
        "\n",
        "# print ('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
        "# print ('==>>> total testing batch number: {}'.format(len(test_loader)))\n",
        "\n",
        "\n",
        "# model = models.resnet18()\n",
        "\n",
        "# epochs = 1\n",
        "# steps = 0\n",
        "# running_loss = 0\n",
        "# print_every = 10\n",
        "# train_losses, test_losses = [], []\n",
        "# for epoch in range(epochs):\n",
        "#     for inputs, labels in trainloader:\n",
        "#         steps += 1\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         logps = model.forward(inputs)\n",
        "#         loss = criterion(logps, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "#             test_loss = 0\n",
        "#             accuracy = 0\n",
        "#             model.eval()\n",
        "#             with torch.no_grad():\n",
        "#                 for inputs, labels in testloader:\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
        "#                     logps = model.forward(inputs)\n",
        "#                     batch_loss = criterion(logps, labels)\n",
        "#                     test_loss += batch_loss.item()\n",
        "                    \n",
        "#                     ps = torch.exp(logps)\n",
        "#                     top_p, top_class = ps.topk(1, dim=1)\n",
        "#                     equals =  top_class == labels.view(*top_class.shape)\n",
        "#                     accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "#             train_losses.append(running_loss/len(trainloader))\n",
        "#             test_losses.append(test_loss/len(testloader))                    \n",
        "#             print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "#                   f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "#                   f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
        "#                   f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
        "#             running_loss = 0\n",
        "#             model.train()\n",
        "# torch.save(model, 'resnet18i.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start glob\n",
            "finish glob\n",
            "finish sort\n",
            "start glob\n",
            "finish glob\n",
            "finish sort\n",
            "==>>> total trainning batch number: 33004\n",
            "==>>> total testing batch number: 33004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-ca70f51091a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-97-aea49acb5047>\", line 32, in __getitem__\n    img = self.transform(img)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 164, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 208, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n"
          ]
        }
      ]
    }
  ]
}